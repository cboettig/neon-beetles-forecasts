---
title: "scores"
output: github_document
---


```{r}
library(tidyverse)
library(neon4cast)
```


```{r}
targets <-
  "https://data.ecoforecast.org/targets/beetles/beetles-targets.csv.gz" %>% 
  read_csv(col_types = "cDdd")

targets <- read_csv("/minio/shared-data/beetles-targets.csv.gz", col_types = "cDdd")

```




```{r}
## Not many new observations since 2021-05-01! 
targets  %>% 
  filter(time > as.Date("2021-05-01")) %>% as_tibble() %>%
  mutate(has_richness = !is.na(richness), has_abund = !is.na(abundance)) %>% 
  count(has_abund, has_richness)

```


Convert the target data to long form (`time`, `siteID`, `target`, `obs`)


```{r}
target_variables <- c("abundance", "richness")

tidy_targets <- targets %>% 
  pivot_longer(any_of(target_variables),
               names_to = "target",
               values_to = "obs")
```

Read in all the forecasts to a combined data.frame, using `id` column from the filename.
Parse filename into `theme`, `issue_date`, and `team`.

```{r}
pattern<- "(\\w+)\\-(\\d{4}\\-\\d{2}\\-\\d{2})\\-(\\w+)\\.csv\\.gz"

forecast <- 
  fs::dir_ls(glob = "beetles*.csv.gz") %>% 
  map_dfr(read_csv,
          show_col_types = FALSE,
          .id = "file") %>% 
  mutate(theme = gsub(pattern, "\\1", file),
         issue_date = gsub(pattern, "\\2", file),
         team = gsub(pattern, "\\3", file))
```

Convert forecast into tidy (long) form

```{r}

## assumes mean-sd form, not appropriate for ensembles
tidy_forecast <- forecast  %>%
  pivot_longer(tidyselect::any_of(target_variables), 
               names_to = "target", values_to = "value") %>%
  pivot_wider(values_from=value, names_from = statistic)
```

To score a forecast, we can simply join the tidy tables and compute the scores.
We may first omit any predictions (target, siteID, time) points where we lack an observation.

```{r}
scored_forecast <- 
  left_join(tidy_forecast, tidy_targets) %>%
  filter(!is.na(obs)) %>% 
  mutate(crps = scoringRules::crps_norm(y = obs, mean = mean, sd = sd),
         logs = scoringRules::logs_norm(y = obs, mean = mean, sd = sd))

```

For bookkeeping, we compute the `forecast_start_time` as being one observation interval before the first forecasted value in each site for each target.  (Because data collection lags behind or is missing in some sites, their forecast start time may be considerably earlier than sites which have more recent available data, even when the forecast for each site is computed at the same time.) 

Once we know the starting time, we can compute the horizon over which any prediction was made.

```{r}
interval <- tidy_forecast %>%
  group_by(theme, team, issue_date, target, siteID) %>% 
  summarise(interval = min(time-lag(time), na.rm=TRUE),
            forecast_start_time = min(time) - interval,
            .groups = "drop")


## add columns for start_time and horizon
scored_forecast <- scored_forecast %>% 
  left_join(interval) %>% 
  mutate(horizon = time - forecast_start_time)


scored_forecast %>% count(horizon, sort=TRUE)
```





````{r}
df <- scored_forecast %>%
  group_by(theme, team, time, siteID) %>%
  slice_min(issue_date) %>%
#  slice_min(horizon) %>% 
  na.omit() #%>% 
#  filter(siteID %in% top_sites) 
  
df %>%
  ggplot(aes(x=time, y = mean, ymin = mean-2*sd, ymax = mean+2*sd, fill=team, col=team)) +
  geom_point(aes(y = obs), col="black") + 
#  geom_ribbon(alpha = 0.2, show.legend=FALSE) +
  geom_linerange(show.legend=FALSE, lwd = 1, alpha=0.8) +
  geom_point() + 
  # geom_line(aes(y = mean)) +
  facet_grid(target~siteID, scales = "free")
```